LLM을 학습시키기 전 데이터 전처리는 모델의 성능을 결정짓는 핵심 단계입니다. 하지만 프리 트레이닝과 파인 튜닝은 목적이 다른 만큼 전처리 방식에도 차이가 있습니다. 

### 1. 프리 트레이닝(Pre-training) 전처리 ###
방대한 양의 비구조화 데이터에서 일반적인 지식을 학습할 수 있게 정제하는 데 집중합니다. 
* 데이터 클리닝: HTML 태그, 광고성 텍스트, 무의미한 특수문자를 제거하고 중복된 문서를 필터링합니다.
* 독성 제거: 혐오 표현이나 비윤리적인 내용을 사전에 탐지하여 제외합니다.
* 토크나이징: 텍스트를 모델이 이해할 수 있는 숫자(토큰) 단위로 쪼개고, Byte Pair Encoding(BPE) 등을 사용해 어휘 사전을 구축합니다. 

### 2. 파인 튜닝(Fine-tuning) 전처리 ###
특정 목적(예: 챗봇, 요약)에 맞게 구조화된 데이터를 만드는 것이 핵심입니다. 
* 데이터 포맷팅: [Instruction] - [Input] - [Output]과 같은 특정 템플릿에 맞춰 데이터를 재구성합니다.
* 품질 관리: 데이터 양보다는 '정답(Label)'의 정확도가 중요하므로, 사람이 직접 검수하거나 고품질의 합성 데이터를 생성합니다.
* 특수 토큰 추가: 시스템 메시지나 사용자/어시스턴트 역할을 구분하기 위한 특수 토큰을 삽입하여 대화 맥락을 이해시킵니다. 


## 특지 및 작업빈드 ##

데이터 전처리는 모델 학습 전 반드시 거쳐야 하는 병목 구간 중 하나입니다. 자원 소모량과 수행 빈도는 데이터의 규모에 따라 극명하게 달라집니다. 
### 1. CPU 자원 소모량: "코어는 다다익선" ###
학습 자체는 GPU가 담당하지만, 텍스트 정제, 토큰화, 중복 제거 같은 전처리는 대부분 CPU가 처리합니다. 
* 요구 사양: 대규모 전처리를 위해서는 고성능 워크스테이션급 이상의 CPU(예: AMD Threadripper PRO 또는 Intel Xeon W)가 권장됩니다.
* 메모리(RAM): CPU 코어당 최소 4GB 이상의 RAM이 뒷받침되어야 병목 없이 병렬 처리가 가능하며, 대형 모델 학습 시에는 128GB~256GB 이상이 필요하기도 합니다.
* 병렬 처리: 전처리는 독립적인 작업(예: 문장별 토큰화)이 많아 코어 수가 많을수록 처리 속도가 선형적으로 빨라집니다. 

### 2. 수행 시간과 빈도: "반복적인 고난의 행군" ###
데이터 전처리는 한 번에 끝나지 않으며, 모델 품질을 높이기 위해 반복적으로 수행됩니다. 
* 수행 기간:
   * 파인 튜닝: 데이터 양이 상대적으로 적어 수 시간에서 수 일 내에 완료됩니다.
* 프리 트레이닝: 테라바이트급 데이터를 처리할 경우, 클러스터 규모에 따라 수 주 이상 걸릴 수도 있습니다.
  * 빈도(자주 하는 정도):
    * 실험 단계: 하이퍼파라미터 변경이나 특정 데이터의 오답 노트를 기반으로 수십 번 이상 전처리를 다시 하여 최적의 조합을 찾습니다.
    * 운영 단계: 새로운 데이터가 쌓일 때마다 주기적으로 재학습(Retraining)을 해야 하므로, 전처리 파이프라인을 자동화하여 지속적으로 가동합니다. 

결론적으로, CPU는 코어 수가 많은 멀티코어 환경일수록 유리하며, 전처리는 학습 품질 만족 시까지 무한 반복되는 반복 작업에 가깝습니다.

## 서버 사양 ##

서버 사양은 "데이터가 한 번에 램(RAM)에 올라가는가?"와 "GPU가 노는 시간(병목)을 얼마나 줄일 것인가?"에 따라 결정됩니다. 구체적인 기준은 다음과 같습니다.
### 1. CPU: 병렬 처리와 데이터 피딩(Feeding) ###
전처리는 CPU의 코어 수가 속도를 결정합니다.
* 기준: 보통 GPU 1장당 4~8개의 CPU 코어를 할당하는 것이 국민 룰입니다.
* 이유: CPU가 토큰화나 증강(Augmentation)을 마쳐야 GPU가 연산을 시작하는데, CPU가 느리면 GPU 점유율이 떨어지는 '병목 현상'이 생깁니다. NVIDIA 가이드에서는 고성능 스토리지와 빠른 CPU의 조합을 강조합니다.

### 2. RAM: 데이터의 크기와 배치 사이즈 ###
데이터를 전처리할 때 메모리가 부족하면 디스크를 쓰게 되어 속도가 수백 배 느려집니다.
* 기준: 다루는 데이터셋 전체 크기의 2~3배 정도의 램을 권장합니다.
* 이유: 전처리 과정에서 원본 데이터, 정제 데이터, 토큰화된 데이터가 동시에 메모리에 상주해야 하기 때문입니다. Hugging Face 공식 문서에서는 대용량 데이터 처리를 위한 메모리 매핑 기법을 권장하기도 합니다.

### 3. GPU: 모델의 파라미터 수 (VRAM) ###
GPU는 전처리가 아니라 실제 학습을 위한 자원입니다.
* 기준: 모델의 파라미터 수 x 4(또는 8) 바이트 이상의 VRAM(비디오램)이 필요합니다.
* 예시: Llama-3 8B 모델을 튜닝하려면 최소 24GB VRAM(RTX 3090/4090급)이 필요하며, 대규모 학습 시에는 A100이나 H100 같은 엔터프라이즈급 GPU가 기준이 됩니다.

### 4. Storage(SSD): 읽기/쓰기 속도 ###
* 기준: 무조건 NVMe SSD입니다.
* 이유: 수백만 개의 작은 텍스트 파일을 읽어야 하므로, 일반 HDD를 쓰면 CPU/GPU 사양이 아무리 좋아도 전처리 단계에서 멈춰있게 됩니다.

## 조합 ##

LLM 학습 환경은 '내 데이터가 GPU 메모리에 들어가는가'와 '전처리 속도를 얼마나 참을 수 있는가'의 싸움입니다. 예산과 목적에 따른 두 가지 확실한 경로를 추천해 드릴게요.
### 1. 가성비 조합 (로컬 워크스테이션 / 소규모 파인 튜닝용) ###
주로 7B~14B 규모의 모델을 QLoRA나 LoRA 기법으로 효율적인 파인 튜닝을 할 때 가장 선호되는 구성입니다.
* CPU: AMD Ryzen 9 7950X (16코어 32스레드) - 멀티코어 성능이 뛰어나 전처리 병렬화에 유리합니다.
* RAM: 64GB ~ 128GB DDR5 - 데이터셋을 램에 올리고 처리하기에 충분합니다.
* GPU: NVIDIA GeForce RTX 4090 (24GB) 1~2장 - 소비자용 중 유일하게 대규모 VRAM을 갖춘 압도적 가성비 카드입니다.
* Storage: 2TB NVMe SSD (PCIe 4.0 이상) - 데이터 로딩 속도 최적화.
* 용도: 개인 연구, 스타트업 내부 데이터 튜닝, 특정 도메인 특화 모델 개발.

### 2. 성능 위주 조합 (엔터프라이즈 / 프리 트레이닝 및 전문 학습용) ###
수천만 건 이상의 데이터를 전처리하고, 수십억 파라미터 모델을 본격적으로 학습시킬 때 필요한 사양입니다.
* CPU: AMD Threadripper PRO 5975WX 또는 Dual Intel Xeon Platinum - 수십 명의 사용자가 동시 접속하거나 방대한 전처리 파이프라인을 돌릴 때 필수적입니다.
* RAM: 512GB ~ 1TB ECC RAM - 데이터 유실 방지와 대규모 데이터셋 상주용.
* GPU: NVIDIA H100 (80GB) 또는 A100 (80GB) 4~8개 (HGX/DGX 시스템) - NVLink를 통해 GPU 간 통신 속도를 극대화하여 대규모 배치 사이즈를 소화합니다.
* Storage: 10TB+ NVMe RAID 구성 - 입출력 병목을 완전히 제거합니다.
* 용도: 파운데이션 모델 프리 트레이닝, 대규모 멀티모달 학습, 실시간 서비스 운영.
