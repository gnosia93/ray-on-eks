워크숍 완성도를 높일 3가지 제안
* C6 - Object Spilling & OOM 방지:
텍스트 데이터는 개별 크기는 작지만 수백만 개가 쌓이면 메모리 압박이 심합니다.
RAY_object_spilling_config를 통해 S3를 스필링 위치로 설정하는 트릭을 넣으시면, 로컬 디스크가 부족한 환경에서도 테라바이트급 데이터를 안전하게 처리하는 실습이 가능해집니다. Ray Object Spilling Guide를 참고해 보세요.

* C8 - 텍스트 전처리 (Docling 연동):
단순 정규식 전처리 외에, 앞서 논의한 Docling 같은 라이브러리를 Ray의 map_batches에 얹어서 "PDF to Clean Markdown" 파이프라인을 실습 예제로 넣으시면 수강생들의 만족도가 매우 높을 것 같습니다.

* C5 - 스팟 인스턴스 전략:
스팟 인스턴스 중단(Preemption) 시 Ray의 Fault Tolerance가 어떻게 작동하는지(상태 복구)를 시각적으로 모니터링(C7)과 연결해 보여주면 "천재들의 설계"를 가장 확실히 체험하는 포인트가 될 것입니다.

----
1. 가비지 제거 (Cleaning) - 필수
텍스트의 품질을 결정하는 '세탁' 단계입니다.
노이즈 제거: HTML 태그, 불필요한 제어 문자, 깨진 유니코드 제거.
광고/상용구 필터링: 웹 크롤링 데이터의 경우 "쿠키 수락", "로그인" 같은 반복적 가비지 제거.
PII 제거: 개인정보(전화번호, 주소) 비식별화.

2. 정규화 (Normalization) - 필수
데이터의 일관성을 맞추는 단계입니다.
형식 통일: 대소문자 통합, 공백 정규화(여러 개의 공백 → 하나로).
유니코드 정규화: 한글 자음/모음 분리 현상 방지(NFC/NFD).

3. 구조화 및 청킹 (Structuring & Chunking) - LLM 특화
단순 텍스트를 LLM이 먹기 좋게 '의미 단위'로 가공하는 과정입니다.
구조 추출: 제목, 본문, 표(Table)를 구분하여 마크다운(Markdown)으로 변환. (여기서 Docling이 활약합니다.)
청킹: 모델의 컨텍스트 제한을 고려해 512~1024 토큰 단위로 자르기.

4. 토큰나이징 (Tokenizing) - 선택/마지막 단계
여기서 주의할 점! 요즘은 전처리 파이프라인에서 토큰나이징을 미리 해서 저장하는 경우보다, 임베딩 직전에 하는 경우가 많습니다.
이유: 사용하는 모델(OpenAI, Llama3 등)마다 사용하는 토크나이저가 다르기 때문입니다.
실무: 전처리 단계에서는 "깨끗한 평문(Clean Text)" 상태로 저장하는 것이 재사용성이 가장 높습니다.


워크숍을 위한 조언: "어디까지 실습에 넣을까요?"
수강생들이 가장 체감하기 좋은 '텍스트 전처리 끝판왕' 코스는 다음과 같습니다.
가비지 제거 + 마크다운 변환 (Docling 사용)
의미 단위 청킹 (LangChain Splitter 사용)
임베딩을 위한 최종 텍스트 리스트 생성
토큰나이징은 "모델에 넣기 전 최종 변환 단계"라는 개념으로 따로 설명하시는 것이 아키텍처적으로 더 깔끔합니다.
C8 챕터에서 이 과정을 Ray Data의 map_batches 하나에 다 때려 넣는 실습을 보여주실 건가요? 아니면 단계를 나눠서 저장하는 방식을 보여주실 건가요? (Ray의 Lazy Evaluation 특징을 보여주기엔 한 번에 연결하는 게 멋지긴 합니다!)
