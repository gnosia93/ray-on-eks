# ray-on-eks


LLM 훈련/파인튜닝 시 Ray가 기존 Kubeflow 방식보다 유리한 포인트 3가지는 이렇습니다:

#### 1. 7B를 넘어 70B 모델로 갈 때 (분산 전략의 유연성) ####
일반적인 PyTorchJob은 단순 데이터 병렬 처리(DDP)에는 강하지만, 모델 병렬화나 복잡한 샤딩 전략을 짤 때는 코드가 지저분해집니다.
Ray는 DeepSpeed나 PyTorch FSDP 설정을 파이썬 코드 몇 줄로 추상화해줍니다. 특히 모델이 커질수록 발생하는 Checkpoints 저장/로드 병목을 Ray Data를 통해 분산 처리하여 시간을 크게 단축합니다.

#### 2. 가변적인 GPU 자원 관리 (Spot Instance 활용) ####
LLM 훈련은 비쌉니다. Ray는 Preemptible/Spot Instance 대응 능력이 뛰어납니다.
EKS에서 Spot 인스턴스가 회수되어 노드가 죽어도, Ray의 Object Store와 상태 관리 기능을 통해 훈련을 자동으로 재개(Fault Tolerance)하는 메커니즘이 Kubeflow보다 훨씬 유연합니다.

#### 3. 데이터 로딩 병목 해결 ####
LLM은 읽어와야 할 텍스트 데이터셋 자체가 거대합니다. GPU는 노는데 CPU에서 전처리하느라 시간이 다 가는 경우가 많죠.
Ray Data를 쓰면 스트리밍 방식으로 데이터를 GPU에 밀어 넣어주기 때문에, 전체 학습 데이터를 다 로드할 필요 없이 훈련과 동시에 전처리를 병렬화할 수 있습니다.


기존 Kubeflow 기반 방식에서 Pre-training 시 겪으셨을 페인 포인트를 Ray가 해결하는 방식입니다.
#### 1. 전처리와 학습의 "심리스한" 병렬화 (Ray Data) ####
* 기존: PyTorch DataLoader는 단일 노드의 CPU/메모리에 의존합니다. 멀티 노드 학습 시 데이터 셔플링이나 로딩 병목이 생겨 GPU 가동률(Util)이 떨어지기 쉽습니다.
* Ray: Ray Data를 사용하면 수 TB의 데이터를 클러스터 전체 CPU 노드에 분산시켜 실시간으로 전처리(Tokenizing, Global Shuffling)하고, 학습 노드의 GPU로 스트리밍합니다. GPU가 데이터 로딩을 기다리는 시간을 0에 수렴하게 만듭니다.

#### 2. 하드웨어 장애 대응 (Fault Tolerance) ####
* 기존: 수백 개의 GPU로 몇 주간 학습하는 Pre-training 중 노드 하나만 죽어도 전체 PyTorchJob이 터지거나 체크포인트부터 수동 재시작해야 합니다.
* Ray: Ray Train은 노드 장애 시 자동으로 가용 노드를 찾아 학습 프로세스를 재구성하고, 가장 최근의 분산 체크포인트에서 복구하는 기능이 더 세밀하게 설계되어 있습니다.

#### 3. 고성능 분산 전략의 통합 ####
* DeepSpeed/FSDP: Pre-training 필수인 Zero-3 전략이나 텐서 병렬화(TP), 파이프라인 병렬화(PP)를 적용할 때, Ray의 DeepSpeed 통합 가이드를 따르면 복잡한 rank 설정이나 네트워킹 구성을 Ray가 내부적으로 알아서 핸들링합니다.

#### 🛠️ 추천 드리는 이행 전략 (EKS 환경) ####
* KubeRay Operator 설치: 기존 EKS 클러스터에 KubeRay를 설치하여 Ray 클러스터를 하나의 커스텀 리소스로 관리하세요.
* 데이터 소스 연결: S3 등에 저장된 대규모 코퍼스를 Ray Data로 읽어 들이는 로직을 먼저 테스트해보세요.
* Hugging Face Accelerate 연동: 이미 HF 기반이시라면 accelerate 설정을 Ray Train이 감싸는 형태로 시작하면 코드 수정을 최소화할 수 있습니다.


GPU 100대 규모의 LLM Pre-training은 인프라의 미세한 병목이 전체 학습 시간을 며칠씩 지연시킬 수 있는 고난도 작업입니다. 이 정도 규모에서 Ray가 기존 Kubeflow 대비 제공하는 핵심 이점과 최적화 전략은 다음과 같습니다.

#### 1. 거대 규모에서의 'Head Node' 부하 관리 ####
* 100대 이상의 노드가 통신할 때 Ray의 관리 노드(Head Node)에 부하가 집중될 수 있습니다.
* 권장 사양: Head Node는 최소 8 CPU, 32GB RAM 이상을 권장하며, 대규모 클러스터에서는 가급적 워커 작업(Task)을 할당하지 않도록 설정(num-cpus: 0)해야 합니다.
* 네트워크: AWS EKS 환경이라면 데이터 전송 병목을 줄이기 위해 r5dn.16xlarge와 같이 아웃바운드 대역폭이 높은 인스턴스를 Head Node로 사용하는 것이 유리합니다. 

#### 2. 고속 데이터 피딩 (Ray Data vs. PyTorch DataLoader) ####
* Pre-training 시 가장 큰 병목은 100대의 GPU가 먹어치우는 대량의 토큰 데이터를 CPU가 제때 전처리해서 밀어 넣어주지 못할 때 발생합니다.
* Ray Data: 수 TB 급의 데이터셋을 클러스터 전체 CPU에 분산시켜 병렬로 읽고, 셔플링(Shuffling)한 뒤 GPU로 스트리밍합니다.
* 이점: 모든 데이터를 메모리에 올릴 필요가 없으며, 훈련과 전처리가 동시에 일어나 GPU 가동률(Util)을 극대화할 수 있습니다. 

#### 3. 체크포인트 및 장애 복구 (Fault Tolerance) ####
* 100대 규모에서는 하드웨어 장애가 통계적으로 빈번하게 발생합니다.
* 자동 복구: Ray Train은 노드 장애 시 자동으로 새로운 워커를 할당하고 훈련 프로세스를 재구성합니다.
* 효율적 체크포인트: Ray Train은 분산된 체크포인트를 병렬로 저장하고 로드하는 최적화 기능을 제공하여, 모델 파라미터가 거대한 LLM pre-training에서 쓰기/읽기 시간을 크게 줄여줍니다. 

#### 4. EKS 위에서의 하이브리드 운영 ####
* 이미 Kubeflow를 쓰고 계시므로, 전체를 갈아엎기보다 KubeRay Operator를 통해 필요한 시점에만 100대 규모의 Ray 클러스터를 동적으로 띄우는 방식을 추천합니다. 
* Spot Instance 활용: Ray는 가용 자원 변화에 유연하게 대응하므로, 비용 절감을 위해 AWS Spot 인스턴스를 섞어 쓰는 시나리오에서 Kubeflow보다 강력한 복원력을 보여줍니다. 
* 결론적으로, OpenAI와 같은 기업들이 대규모 모델 학습에 Ray를 사용하는 이유는 "수백 대의 GPU를 하나의 거대한 파이썬 인터프리터처럼 다룰 수 있는 유연성" 때문입니다. 
* 현재 DeepSpeed나 FSDP 중 어떤 분산 전략을 주로 검토하고 계신가요? 전략에 따른 세부 설정이 학습 효율에 결정적인 영향을 미칩니다.

LLM Pre-training 과정에서 GPU가 연산을 시작하기 전, CPU 부대가 매달려야 하는 '노가다' 작업은 크게 3가지입니다. 데이터가 TB 단위라면 이 작업이 GPU를 기다리게 만드는 주범이 됩니다.

#### 1. 전역 셔플링 (Global Shuffling) - 가장 무거운 작업 ####
학습의 편향을 막기 위해 수천만 개의 파일을 무작위로 섞어야 합니다.
* 일반 방식: 각 노드가 자기가 가진 데이터 안에서만 섞습니다(Local Shuffle). 이는 학습 품질을 떨어뜨립니다.
* Ray 방식: 수백 대의 CPU 노드가 협력하여 전체 데이터셋을 거대하게 섞습니다. 이때 메모리 간 데이터 교환(All-to-all communication)이 발생하는데, 이 통신 제어를 CPU가 수행합니다.

#### 2. 동적 토큰화 (On-the-fly Tokenization) ####
S3에서 가져온 원시 텍스트(JSONL 등)를 모델이 이해하는 숫자(ID)로 바꾸는 과정입니다.
* 작업 내용: 텍스트 클리닝(특수문자 제거), 정규식 처리, BPE/SentencePiece 인코딩.
* 병목 이유: 모델이 커질수록 시퀀스 길이(Context Length)가 길어지는데(예: 8k, 32k), 이 긴 문장을 실시간으로 자르고 패딩(Padding)하고 마스킹(Masking)하는 계산은 100% CPU의 몫입니다.

#### 3. 데이터 복원 및 스트리밍 해제 ####
* 대규모 데이터셋은 대역폭을 아끼기 위해 보통 WebDataset(.tar)이나 Parquet 등 압축된 형태로 저장됩니다.
* CPU의 역할: S3에서 스트리밍으로 넘어오는 압축 데이터를 실시간으로 풀고(Decompression), 역직렬화(Deserialization)하여 메모리에 텐서 형태로 올립니다.

## 데이터 전처리 ##
파이토치로 100대 규모의 Pre-training을 할 때, 데이터 로딩은 보통 "각 노드의 독립적인 무한 반복" 구조입니다. 이 방식이 100대 규모에서 왜 한계에 부딪히는지 상세히 짚어드릴게요.

#### 1. 데이터 소스 접근 방식: "각자도생" ####
* 일반적인 DistributedDataParallel (DDP) 환경에서는 모든 GPU 워커가 각자의 DistributedSampler를 가집니다.
* 동일 소스, 다른 영역: 100대 노드가 모두 S3(또는 FSx)라는 하나의 소스를 바라보지만, 인덱스 계산(dataset_size // 100)을 통해 자기가 읽어야 할 부분만 각자 가져옵니다.
* 파일 IO 폭증: 100대의 노드에서 각각 num_workers=16 정도로 설정하면, 순간적으로 1,600개의 프로세스가 S3에 동시에 접속해 데이터를 당겨오려고 시도합니다. 이때 S3의 Connection Limit에 걸리거나 네트워크 IO * 병목이 발생해 특정 노드가 느려지는 현상이 빈번합니다.

#### 2. 전처리(CPU) 병목: "GPU 노드 자원 갉아먹기" ####
파이토치는 기본적으로 학습 프로세스와 로더 프로세스가 같은 머신(Node)에 있습니다.
* 자원 경합: GPU가 연산을 수행하는 동안, 같은 노드의 CPU 코어들은 다음 배치를 위해 Tokenizing, Augmentation, Padding을 수행합니다.
* 문제점: Pre-training용 텍스트 데이터는 정규화나 복잡한 토큰화 과정이 필요한데, 이 CPU 점유율이 높아지면 GPU로 명령을 보내는 파이토치 메인 프로세스가 지연(Stall)됩니다. 결과적으로 비싼 GPU가 CPU 작업을 기다리며 놀게 됩니다.

#### 3. Shuffling의 한계: "우물 안 개구리" ####
진정한 Pre-training을 위해서는 전체 데이터셋(수 TB)을 골고루 섞어야 합니다.
* Local Shuffle: 하지만 파이토치는 보통 메모리 한계 때문에 각 노드가 가져온 일부 데이터 내에서만 섞습니다. 이를 보완하려고 학습 전에 미리 데이터를 물리적으로 섞어서 S3에 저장해두기도 하지만, 이는 스토리지 비용과 관리 오버헤드를 엄청나게 발생시킵니다.
* WebDataset/IterableDataset: 이 한계를 극복하려 WebDataset 같은 라이브러리를 써서 스트리밍으로 읽지만, 이 역시 노드 단위의 셔플링이라는 본질적 한계는 여전합니다.

#### 4. Barrier(동기화)로 인한 하향 평준화 ####
DDP는 매 스텝마다 모든 노드의 그래디언트를 맞추는 All-Reduce 과정을 거칩니다.
* 지연의 전이: 만약 100대 중 단 1대의 노드만 데이터 로딩이 늦어지면(S3 응답 지연이나 CPU 부하 등), 나머지 99대의 고성능 GPU는 그 노드가 데이터를 다 읽을 때까지 아무것도 못 하고 기다립니다. 100대 규모에서는 이 "가장 느린 노드"가 매번 발생하기 때문에 전체 효율이 급격히 떨어집니다
