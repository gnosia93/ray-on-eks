# ray-on-eks


LLM 훈련/파인튜닝 시 Ray가 기존 Kubeflow 방식보다 유리한 포인트 3가지는 이렇습니다:

#### 1. 7B를 넘어 70B 모델로 갈 때 (분산 전략의 유연성) ####
일반적인 PyTorchJob은 단순 데이터 병렬 처리(DDP)에는 강하지만, 모델 병렬화나 복잡한 샤딩 전략을 짤 때는 코드가 지저분해집니다.
Ray는 DeepSpeed나 PyTorch FSDP 설정을 파이썬 코드 몇 줄로 추상화해줍니다. 특히 모델이 커질수록 발생하는 Checkpoints 저장/로드 병목을 Ray Data를 통해 분산 처리하여 시간을 크게 단축합니다.

#### 2. 가변적인 GPU 자원 관리 (Spot Instance 활용) ####
LLM 훈련은 비쌉니다. Ray는 Preemptible/Spot Instance 대응 능력이 뛰어납니다.
EKS에서 Spot 인스턴스가 회수되어 노드가 죽어도, Ray의 Object Store와 상태 관리 기능을 통해 훈련을 자동으로 재개(Fault Tolerance)하는 메커니즘이 Kubeflow보다 훨씬 유연합니다.

#### 3. 데이터 로딩 병목 해결 ####
LLM은 읽어와야 할 텍스트 데이터셋 자체가 거대합니다. GPU는 노는데 CPU에서 전처리하느라 시간이 다 가는 경우가 많죠.
Ray Data를 쓰면 스트리밍 방식으로 데이터를 GPU에 밀어 넣어주기 때문에, 전체 학습 데이터를 다 로드할 필요 없이 훈련과 동시에 전처리를 병렬화할 수 있습니다.


기존 Kubeflow 기반 방식에서 Pre-training 시 겪으셨을 페인 포인트를 Ray가 해결하는 방식입니다.
#### 1. 전처리와 학습의 "심리스한" 병렬화 (Ray Data) ####
* 기존: PyTorch DataLoader는 단일 노드의 CPU/메모리에 의존합니다. 멀티 노드 학습 시 데이터 셔플링이나 로딩 병목이 생겨 GPU 가동률(Util)이 떨어지기 쉽습니다.
* Ray: Ray Data를 사용하면 수 TB의 데이터를 클러스터 전체 CPU 노드에 분산시켜 실시간으로 전처리(Tokenizing, Global Shuffling)하고, 학습 노드의 GPU로 스트리밍합니다. GPU가 데이터 로딩을 기다리는 시간을 0에 수렴하게 만듭니다.

#### 2. 하드웨어 장애 대응 (Fault Tolerance) ####
* 기존: 수백 개의 GPU로 몇 주간 학습하는 Pre-training 중 노드 하나만 죽어도 전체 PyTorchJob이 터지거나 체크포인트부터 수동 재시작해야 합니다.
* Ray: Ray Train은 노드 장애 시 자동으로 가용 노드를 찾아 학습 프로세스를 재구성하고, 가장 최근의 분산 체크포인트에서 복구하는 기능이 더 세밀하게 설계되어 있습니다.

#### 3. 고성능 분산 전략의 통합 ####
* DeepSpeed/FSDP: Pre-training 필수인 Zero-3 전략이나 텐서 병렬화(TP), 파이프라인 병렬화(PP)를 적용할 때, Ray의 DeepSpeed 통합 가이드를 따르면 복잡한 rank 설정이나 네트워킹 구성을 Ray가 내부적으로 알아서 핸들링합니다.

#### 🛠️ 추천 드리는 이행 전략 (EKS 환경) ####
* KubeRay Operator 설치: 기존 EKS 클러스터에 KubeRay를 설치하여 Ray 클러스터를 하나의 커스텀 리소스로 관리하세요.
* 데이터 소스 연결: S3 등에 저장된 대규모 코퍼스를 Ray Data로 읽어 들이는 로직을 먼저 테스트해보세요.
* Hugging Face Accelerate 연동: 이미 HF 기반이시라면 accelerate 설정을 Ray Train이 감싸는 형태로 시작하면 코드 수정을 최소화할 수 있습니다.


GPU 100대 규모의 LLM Pre-training은 인프라의 미세한 병목이 전체 학습 시간을 며칠씩 지연시킬 수 있는 고난도 작업입니다. 이 정도 규모에서 Ray가 기존 Kubeflow 대비 제공하는 핵심 이점과 최적화 전략은 다음과 같습니다.

#### 1. 거대 규모에서의 'Head Node' 부하 관리 ####
* 100대 이상의 노드가 통신할 때 Ray의 관리 노드(Head Node)에 부하가 집중될 수 있습니다.
* 권장 사양: Head Node는 최소 8 CPU, 32GB RAM 이상을 권장하며, 대규모 클러스터에서는 가급적 워커 작업(Task)을 할당하지 않도록 설정(num-cpus: 0)해야 합니다.
* 네트워크: AWS EKS 환경이라면 데이터 전송 병목을 줄이기 위해 r5dn.16xlarge와 같이 아웃바운드 대역폭이 높은 인스턴스를 Head Node로 사용하는 것이 유리합니다. 

#### 2. 고속 데이터 피딩 (Ray Data vs. PyTorch DataLoader) ####
* Pre-training 시 가장 큰 병목은 100대의 GPU가 먹어치우는 대량의 토큰 데이터를 CPU가 제때 전처리해서 밀어 넣어주지 못할 때 발생합니다.
* Ray Data: 수 TB 급의 데이터셋을 클러스터 전체 CPU에 분산시켜 병렬로 읽고, 셔플링(Shuffling)한 뒤 GPU로 스트리밍합니다.
* 이점: 모든 데이터를 메모리에 올릴 필요가 없으며, 훈련과 전처리가 동시에 일어나 GPU 가동률(Util)을 극대화할 수 있습니다. 

#### 3. 체크포인트 및 장애 복구 (Fault Tolerance) ####
* 100대 규모에서는 하드웨어 장애가 통계적으로 빈번하게 발생합니다.
* 자동 복구: Ray Train은 노드 장애 시 자동으로 새로운 워커를 할당하고 훈련 프로세스를 재구성합니다.
* 효율적 체크포인트: Ray Train은 분산된 체크포인트를 병렬로 저장하고 로드하는 최적화 기능을 제공하여, 모델 파라미터가 거대한 LLM pre-training에서 쓰기/읽기 시간을 크게 줄여줍니다. 

#### 4. EKS 위에서의 하이브리드 운영 ####
* 이미 Kubeflow를 쓰고 계시므로, 전체를 갈아엎기보다 KubeRay Operator를 통해 필요한 시점에만 100대 규모의 Ray 클러스터를 동적으로 띄우는 방식을 추천합니다. 
* Spot Instance 활용: Ray는 가용 자원 변화에 유연하게 대응하므로, 비용 절감을 위해 AWS Spot 인스턴스를 섞어 쓰는 시나리오에서 Kubeflow보다 강력한 복원력을 보여줍니다. 
* 결론적으로, OpenAI와 같은 기업들이 대규모 모델 학습에 Ray를 사용하는 이유는 "수백 대의 GPU를 하나의 거대한 파이썬 인터프리터처럼 다룰 수 있는 유연성" 때문입니다. 
* 현재 DeepSpeed나 FSDP 중 어떤 분산 전략을 주로 검토하고 계신가요? 전략에 따른 세부 설정이 학습 효율에 결정적인 영향을 미칩니다.

LLM Pre-training 과정에서 GPU가 연산을 시작하기 전, CPU 부대가 매달려야 하는 '노가다' 작업은 크게 3가지입니다. 데이터가 TB 단위라면 이 작업이 GPU를 기다리게 만드는 주범이 됩니다.

#### 1. 전역 셔플링 (Global Shuffling) - 가장 무거운 작업 ####
학습의 편향을 막기 위해 수천만 개의 파일을 무작위로 섞어야 합니다.
* 일반 방식: 각 노드가 자기가 가진 데이터 안에서만 섞습니다(Local Shuffle). 이는 학습 품질을 떨어뜨립니다.
* Ray 방식: 수백 대의 CPU 노드가 협력하여 전체 데이터셋을 거대하게 섞습니다. 이때 메모리 간 데이터 교환(All-to-all communication)이 발생하는데, 이 통신 제어를 CPU가 수행합니다.

#### 2. 동적 토큰화 (On-the-fly Tokenization) ####
S3에서 가져온 원시 텍스트(JSONL 등)를 모델이 이해하는 숫자(ID)로 바꾸는 과정입니다.
* 작업 내용: 텍스트 클리닝(특수문자 제거), 정규식 처리, BPE/SentencePiece 인코딩.
* 병목 이유: 모델이 커질수록 시퀀스 길이(Context Length)가 길어지는데(예: 8k, 32k), 이 긴 문장을 실시간으로 자르고 패딩(Padding)하고 마스킹(Masking)하는 계산은 100% CPU의 몫입니다.

#### 3. 데이터 복원 및 스트리밍 해제 ####
* 대규모 데이터셋은 대역폭을 아끼기 위해 보통 WebDataset(.tar)이나 Parquet 등 압축된 형태로 저장됩니다.
* CPU의 역할: S3에서 스트리밍으로 넘어오는 압축 데이터를 실시간으로 풀고(Decompression), 역직렬화(Deserialization)하여 메모리에 텐서 형태로 올립니다.

## 데이터 전처리 ##
파이토치로 100대 규모의 Pre-training을 할 때, 데이터 로딩은 보통 "각 노드의 독립적인 무한 반복" 구조입니다. 이 방식이 100대 규모에서 왜 한계에 부딪히는지 상세히 짚어드릴게요.

#### 1. 데이터 소스 접근 방식: "각자도생" ####
* 일반적인 DistributedDataParallel (DDP) 환경에서는 모든 GPU 워커가 각자의 DistributedSampler를 가집니다.
* 동일 소스, 다른 영역: 100대 노드가 모두 S3(또는 FSx)라는 하나의 소스를 바라보지만, 인덱스 계산(dataset_size // 100)을 통해 자기가 읽어야 할 부분만 각자 가져옵니다.
* 파일 IO 폭증: 100대의 노드에서 각각 num_workers=16 정도로 설정하면, 순간적으로 1,600개의 프로세스가 S3에 동시에 접속해 데이터를 당겨오려고 시도합니다. 이때 S3의 Connection Limit에 걸리거나 네트워크 IO * 병목이 발생해 특정 노드가 느려지는 현상이 빈번합니다.

#### 2. 전처리(CPU) 병목: "GPU 노드 자원 갉아먹기" ####
파이토치는 기본적으로 학습 프로세스와 로더 프로세스가 같은 머신(Node)에 있습니다.
* 자원 경합: GPU가 연산을 수행하는 동안, 같은 노드의 CPU 코어들은 다음 배치를 위해 Tokenizing, Augmentation, Padding을 수행합니다.
* 문제점: Pre-training용 텍스트 데이터는 정규화나 복잡한 토큰화 과정이 필요한데, 이 CPU 점유율이 높아지면 GPU로 명령을 보내는 파이토치 메인 프로세스가 지연(Stall)됩니다. 결과적으로 비싼 GPU가 CPU 작업을 기다리며 놀게 됩니다.

#### 3. Shuffling의 한계: "우물 안 개구리" ####
진정한 Pre-training을 위해서는 전체 데이터셋(수 TB)을 골고루 섞어야 합니다.
* Local Shuffle: 하지만 파이토치는 보통 메모리 한계 때문에 각 노드가 가져온 일부 데이터 내에서만 섞습니다. 이를 보완하려고 학습 전에 미리 데이터를 물리적으로 섞어서 S3에 저장해두기도 하지만, 이는 스토리지 비용과 관리 오버헤드를 엄청나게 발생시킵니다.
* WebDataset/IterableDataset: 이 한계를 극복하려 WebDataset 같은 라이브러리를 써서 스트리밍으로 읽지만, 이 역시 노드 단위의 셔플링이라는 본질적 한계는 여전합니다.

#### 4. Barrier(동기화)로 인한 하향 평준화 ####
DDP는 매 스텝마다 모든 노드의 그래디언트를 맞추는 All-Reduce 과정을 거칩니다.
* 지연의 전이: 만약 100대 중 단 1대의 노드만 데이터 로딩이 늦어지면(S3 응답 지연이나 CPU 부하 등), 나머지 99대의 고성능 GPU는 그 노드가 데이터를 다 읽을 때까지 아무것도 못 하고 기다립니다. 100대 규모에서는 이 "가장 느린 노드"가 매번 발생하기 때문에 전체 효율이 급격히 떨어집니다

## 데이터 로딩 배리어 ##
일반적인 파이토치 코드를 보시면 "모든 전처리가 끝날 때까지 기다려라"라는 명시적인 명령은 없습니다. 하지만 DataLoader의 내부 메커니즘과 DDP의 동기화 구조 때문에 실제로는 모두가 보조를 맞춰야만 진행되는 '암묵적 대기'가 발생합니다.
구체적으로 어떻게 동작하는지 뜯어보겠습니다.

#### 1. "Lazy Loading"의 함정 ####
파이토치의 DataLoader는 데이터를 미리 다 만들어두지 않고, 학습 직전에 필요한 만큼만 가져오는 Lazy(게으른) 방식입니다.
* 첫 번째 배치의 저주: 학습 루프(for batch in dataloader:)가 시작되는 순간, 100대 노드의 CPU 워커들이 일제히 전처리를 시작합니다.
* 준비 시간의 차이: 어떤 노드는 텍스트가 짧아 전처리가 0.1초 만에 끝나고, 어떤 노드는 문장이 길거나 S3 응답이 늦어 0.5초가 걸립니다.

#### 2. DDP의 'All-Reduce'가 만드는 강제 동기화 ####
파이토치 분산 훈련(DistributedDataParallel)의 핵심은 "모든 노드가 동일한 타이밍에 그래디언트를 주고받아야 한다"는 것입니다.
* 상황: 0번 노드는 전처리가 빨리 끝나서 GPU 연산을 0.2초 만에 마쳤습니다. 그런데 99번 노드는 데이터 로딩이 늦어져서 아직 GPU 연산을 시작도 못 했습니다.
* 결과: 0번 노드는 자기 연산이 끝났어도 optimizer.step() 단계에서 99번 노드의 결과가 올 때까지 네트워크 상에서 무한 대기(Blocking)합니다.
즉, 코딩은 각자 하는 것처럼 보이지만, 하드웨어 레벨에서는 가장 느린 노드의 전처리 속도에 전체 100대의 속도가 맞춰지는 하향 평준화가 매 스텝 발생합니다.

#### 3. "Prefetching"도 한계가 있다 ####
보통 prefetch_factor 옵션을 써서 다음 배치를 미리 전처리해두기도 합니다.
하지만 100대 규모에서는 누적된 지연이 발생합니다. 특정 스텝에서 데이터 로딩이 튀면(S3 타임아웃 등), 그 노드는 prefetch해둔 데이터를 다 써버리고 결국 GPU를 놀리게 됩니다. 그러면 다시 100대 전체가 멈춥니다.

---

LLM Pre-training처럼 거대한 모델을 학습할 때 셔플링(Shuffling)을 생략하거나 대충 하는 것은 "수억 원짜리 GPU를 돌려 쓰레기(Garbage)를 만드는 것"과 다름없을 만큼 치명적입니다.
그 이유를 3가지 핵심 포인트로 짚어 드릴게요.
#### 1. 모델의 '편향된 기억' (Catastrophic Forgetting) ####
LLM 데이터셋은 보통 출처별(위키피디아, 코드, 뉴스, 논문 등)로 묶여 있습니다.
* 셔플링 안 하면: 모델이 한동안 '뉴스'만 읽다가, 그다음엔 '코드'만 읽고, 그다음엔 '수학 논문'만 읽게 됩니다.
* 결과: 코드를 학습할 때쯤이면 앞에서 배운 뉴스의 문체나 지식을 잊어버립니다. 이를 치명적 망각이라 부르며, 모델이 전반적인 지능을 갖추지 못하고 마지막에 읽은 데이터의 특성에만 매몰됩니다.

#### 2. 가중치 업데이트의 '진동' (Loss Instability) ####
학습은 손실 함수(Loss)의 골짜기를 따라 최저점을 찾아가는 과정입니다.
* 셔플링 안 하면: 비슷한 데이터가 연속으로 들어오면 그래디언트(기울기)가 한 방향으로만 계속 쏠립니다. 그러다 갑자기 다른 성격의 데이터 뭉치가 나오면 가중치가 급격하게 튀어버립니다.
* 결과: PyTorch DDP 환경에서 Loss가 수렴하지 않고 요동치거나 발산(Explode)하여 학습이 중단되는 주된 원인이 됩니다.

#### 3. '중복 데이터'의 과적합 (Overfitting) ####
인터넷 데이터셋에는 중복되거나 유사한 문서가 많습니다.
* 셔플링 안 하면: 특정 도메인의 유사한 문장들을 연속해서 학습하게 되고, 모델은 이를 '매우 중요한 진리'로 오해하여 통째로 암기해 버립니다.
* 결과: 모델이 새로운 질문에 대답하는 대신, 특정 웹사이트 문구를 그대로 읊는 등의 부작용이 발생합니다.

#### 💡 그럼 어떻게 해결하고 계신가요? ####
보통 Ray 같은 도구가 없으면 엔지니어들은 다음 중 하나를 선택합니다:
* 노가다 방식: 학습 전에 S3에 있는 TB 단위 데이터를 미리 CPU로 다 섞어서 새로 저장합니다. (저장 비용 + 컴퓨팅 비용 발생)
* 타협 방식 (현재 하시는 방식일 확률 높음): 파이토치에서 아주 작은 윈도우 사이즈로 '흉내'만 냅니다. (학습 품질 저하)

