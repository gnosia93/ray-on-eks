# ray-on-eks


LLM 훈련/파인튜닝 시 Ray가 기존 Kubeflow 방식보다 유리한 포인트 3가지는 이렇습니다:

#### 1. 7B를 넘어 70B 모델로 갈 때 (분산 전략의 유연성) ####
일반적인 PyTorchJob은 단순 데이터 병렬 처리(DDP)에는 강하지만, 모델 병렬화나 복잡한 샤딩 전략을 짤 때는 코드가 지저분해집니다.
Ray는 DeepSpeed나 PyTorch FSDP 설정을 파이썬 코드 몇 줄로 추상화해줍니다. 특히 모델이 커질수록 발생하는 Checkpoints 저장/로드 병목을 Ray Data를 통해 분산 처리하여 시간을 크게 단축합니다.

#### 2. 가변적인 GPU 자원 관리 (Spot Instance 활용) ####
LLM 훈련은 비쌉니다. Ray는 Preemptible/Spot Instance 대응 능력이 뛰어납니다.
EKS에서 Spot 인스턴스가 회수되어 노드가 죽어도, Ray의 Object Store와 상태 관리 기능을 통해 훈련을 자동으로 재개(Fault Tolerance)하는 메커니즘이 Kubeflow보다 훨씬 유연합니다.

#### 3. 데이터 로딩 병목 해결 ####
LLM은 읽어와야 할 텍스트 데이터셋 자체가 거대합니다. GPU는 노는데 CPU에서 전처리하느라 시간이 다 가는 경우가 많죠.
Ray Data를 쓰면 스트리밍 방식으로 데이터를 GPU에 밀어 넣어주기 때문에, 전체 학습 데이터를 다 로드할 필요 없이 훈련과 동시에 전처리를 병렬화할 수 있습니다.


기존 Kubeflow 기반 방식에서 Pre-training 시 겪으셨을 페인 포인트를 Ray가 해결하는 방식입니다.
#### 1. 전처리와 학습의 "심리스한" 병렬화 (Ray Data) ####
* 기존: PyTorch DataLoader는 단일 노드의 CPU/메모리에 의존합니다. 멀티 노드 학습 시 데이터 셔플링이나 로딩 병목이 생겨 GPU 가동률(Util)이 떨어지기 쉽습니다.
* Ray: Ray Data를 사용하면 수 TB의 데이터를 클러스터 전체 CPU 노드에 분산시켜 실시간으로 전처리(Tokenizing, Global Shuffling)하고, 학습 노드의 GPU로 스트리밍합니다. GPU가 데이터 로딩을 기다리는 시간을 0에 수렴하게 만듭니다.

#### 2. 하드웨어 장애 대응 (Fault Tolerance) ####
* 기존: 수백 개의 GPU로 몇 주간 학습하는 Pre-training 중 노드 하나만 죽어도 전체 PyTorchJob이 터지거나 체크포인트부터 수동 재시작해야 합니다.
* Ray: Ray Train은 노드 장애 시 자동으로 가용 노드를 찾아 학습 프로세스를 재구성하고, 가장 최근의 분산 체크포인트에서 복구하는 기능이 더 세밀하게 설계되어 있습니다.

#### 3. 고성능 분산 전략의 통합 ####
* DeepSpeed/FSDP: Pre-training 필수인 Zero-3 전략이나 텐서 병렬화(TP), 파이프라인 병렬화(PP)를 적용할 때, Ray의 DeepSpeed 통합 가이드를 따르면 복잡한 rank 설정이나 네트워킹 구성을 Ray가 내부적으로 알아서 핸들링합니다.

#### 🛠️ 추천 드리는 이행 전략 (EKS 환경) ####
* KubeRay Operator 설치: 기존 EKS 클러스터에 KubeRay를 설치하여 Ray 클러스터를 하나의 커스텀 리소스로 관리하세요.
* 데이터 소스 연결: S3 등에 저장된 대규모 코퍼스를 Ray Data로 읽어 들이는 로직을 먼저 테스트해보세요.
* Hugging Face Accelerate 연동: 이미 HF 기반이시라면 accelerate 설정을 Ray Train이 감싸는 형태로 시작하면 코드 수정을 최소화할 수 있습니다.


GPU 100대 규모의 LLM Pre-training은 인프라의 미세한 병목이 전체 학습 시간을 며칠씩 지연시킬 수 있는 고난도 작업입니다. 이 정도 규모에서 Ray가 기존 Kubeflow 대비 제공하는 핵심 이점과 최적화 전략은 다음과 같습니다.

#### 1. 거대 규모에서의 'Head Node' 부하 관리 ####
* 100대 이상의 노드가 통신할 때 Ray의 관리 노드(Head Node)에 부하가 집중될 수 있습니다.
* 권장 사양: Head Node는 최소 8 CPU, 32GB RAM 이상을 권장하며, 대규모 클러스터에서는 가급적 워커 작업(Task)을 할당하지 않도록 설정(num-cpus: 0)해야 합니다.
* 네트워크: AWS EKS 환경이라면 데이터 전송 병목을 줄이기 위해 r5dn.16xlarge와 같이 아웃바운드 대역폭이 높은 인스턴스를 Head Node로 사용하는 것이 유리합니다. 

#### 2. 고속 데이터 피딩 (Ray Data vs. PyTorch DataLoader) ####
* Pre-training 시 가장 큰 병목은 100대의 GPU가 먹어치우는 대량의 토큰 데이터를 CPU가 제때 전처리해서 밀어 넣어주지 못할 때 발생합니다.
* Ray Data: 수 TB 급의 데이터셋을 클러스터 전체 CPU에 분산시켜 병렬로 읽고, 셔플링(Shuffling)한 뒤 GPU로 스트리밍합니다.
* 이점: 모든 데이터를 메모리에 올릴 필요가 없으며, 훈련과 전처리가 동시에 일어나 GPU 가동률(Util)을 극대화할 수 있습니다. 

#### 3. 체크포인트 및 장애 복구 (Fault Tolerance) ####
* 100대 규모에서는 하드웨어 장애가 통계적으로 빈번하게 발생합니다.
* 자동 복구: Ray Train은 노드 장애 시 자동으로 새로운 워커를 할당하고 훈련 프로세스를 재구성합니다.
* 효율적 체크포인트: Ray Train은 분산된 체크포인트를 병렬로 저장하고 로드하는 최적화 기능을 제공하여, 모델 파라미터가 거대한 LLM pre-training에서 쓰기/읽기 시간을 크게 줄여줍니다. 

#### 4. EKS 위에서의 하이브리드 운영 ####
* 이미 Kubeflow를 쓰고 계시므로, 전체를 갈아엎기보다 KubeRay Operator를 통해 필요한 시점에만 100대 규모의 Ray 클러스터를 동적으로 띄우는 방식을 추천합니다. 
* Spot Instance 활용: Ray는 가용 자원 변화에 유연하게 대응하므로, 비용 절감을 위해 AWS Spot 인스턴스를 섞어 쓰는 시나리오에서 Kubeflow보다 강력한 복원력을 보여줍니다. 
* 결론적으로, OpenAI와 같은 기업들이 대규모 모델 학습에 Ray를 사용하는 이유는 "수백 대의 GPU를 하나의 거대한 파이썬 인터프리터처럼 다룰 수 있는 유연성" 때문입니다. 
* 현재 DeepSpeed나 FSDP 중 어떤 분산 전략을 주로 검토하고 계신가요? 전략에 따른 세부 설정이 학습 효율에 결정적인 영향을 미칩니다.


